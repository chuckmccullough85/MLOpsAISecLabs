{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "52c53f0b",
   "metadata": {},
   "source": [
    "\n",
    "Lab: Search Runs\n",
    "================\n",
    "\n",
    "| | |\n",
    "| --------- | --------------------------- |\n",
    "| Notebook  | 8_MLFlowSearch.ipynb    |\n",
    "| Builds On | none |\n",
    "| Time to complete | 30 minutes\n",
    "\n",
    "This lab will walk you through how to search your MLflow runs through\n",
    "the MLflow UI and Python API. This resource will be valuable if you're\n",
    "interested in querying specific runs based on their metrics, params,\n",
    "tags, dataset information, or run metadata.\n",
    "\n",
    "In short, you can leverage SQL-like syntax to filter your runs based on\n",
    "a variety of conditions. Note that the `OR` keyword is not supported and there are a few other\n",
    "differences from SQL mentioned below, but despite these limitations, the\n",
    "run search functionality is quite powerful.\n",
    "\n",
    "Search Runs on MLflow UI\n",
    "--------------------------\n",
    "\n",
    "\n",
    "The MLflow UI provides a powerful search interface that allows you to\n",
    "filter runs. Below we'll...\n",
    "\n",
    "1. Create example MLflow runs\n",
    "2. Look at a simple querying example\n",
    "3. Deep dive into query syntax\n",
    "4. Provide a variety of example queries\n",
    "\n",
    "\n",
    "### Create Example MLflow Runs\n",
    "\n",
    "First, let's create some example MLflow runs. This documentation is\n",
    "based on experiments created with the below script. If you don't want to\n",
    "interactively explore this on your machine, skip this section.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e657c301-4b25-4965-8584-224625f192ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "import numpy as np\n",
    "\n",
    "mlflow.set_experiment(\"search-run-guide\")\n",
    "\n",
    "accuracy = np.arange(0, 1, 0.1)\n",
    "loss = np.arange(1, 0, -0.1)\n",
    "log_scale_loss = np.log(loss)\n",
    "f1_score = np.arange(0, 1, 0.1)\n",
    "\n",
    "batch_size = [2] * 5 + [4] * 5\n",
    "learning_rate = [0.001, 0.01] * 5\n",
    "model = [\"GPT-2\", \"GPT-3\", \"GPT-3.5\", \"GPT-4\"] + [None] * 6\n",
    "\n",
    "task = [\"classification\", \"regression\", \"causal lm\"] + [None] * 7\n",
    "environment = [\"notebook\"] * 5 + [None] * 5\n",
    "\n",
    "dataset_name = [\"custom\"] * 5 + [\"also custom\"] * 5\n",
    "dataset_digest = [\"s8ds293b\", \"jks834s2\"] + [None] * 8\n",
    "dataset_context = [\"train\"] * 5 + [\"test\"] * 5\n",
    "\n",
    "for i in range(10):\n",
    "    with mlflow.start_run():\n",
    "        mlflow.log_metrics(\n",
    "            {\n",
    "                \"loss\": loss[i],\n",
    "                \"accuracy\": accuracy[i],\n",
    "                \"log-scale-loss\": log_scale_loss[i],\n",
    "                \"f1 score\": f1_score[i],\n",
    "            }\n",
    "        )\n",
    "\n",
    "        mlflow.log_params(\n",
    "            {\n",
    "                \"batch_size\": batch_size[i],\n",
    "                \"learning rate\": learning_rate[i],\n",
    "                \"model\": model[i],\n",
    "            }\n",
    "        )\n",
    "\n",
    "        mlflow.set_tags(\n",
    "            {\n",
    "                \"task\": task[i],\n",
    "                \"environment\": environment[i],\n",
    "            }\n",
    "        )\n",
    "\n",
    "        dataset = mlflow.data.from_numpy(\n",
    "            features=np.random.uniform(size=[20, 28, 28, 3]),\n",
    "            targets=np.random.randint(0, 10, size=[20]),\n",
    "            name=dataset_name[i],\n",
    "            digest=dataset_digest[i],\n",
    "        )\n",
    "        mlflow.log_input(dataset, context=dataset_context[i])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "463f6ffc",
   "metadata": {},
   "source": [
    "The code above creates 10 MLflow runs with different metrics, params,\n",
    "tags and dataset information. After successful execution, if you return\n",
    "to the MLflow UI in your browser, you should find all of these runs\n",
    "under the experiment \"search-run-guide\", as shown by the following\n",
    "screenshot:\n",
    "\n",
    "\n",
    "[![testing\n",
    "runs](./images//created_mlflow_runs.png)](./images//created_mlflow_runs.png)\n",
    "\n",
    "\n",
    "In real-world production deployments of MLflow, it's common to have\n",
    "thousands or even hundreds of thousands of runs. In such cases, it's\n",
    "important to be able to filter and search for runs based on specific\n",
    "criteria.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d24a084e",
   "metadata": {},
   "source": [
    "### Search Query Example\n",
    "\n",
    "In order to filter your MLflow runs, you will need to write **search\n",
    "queries**, which are pseudo-SQL conditions expressed in a distinct\n",
    "syntax.\n",
    "\n",
    "To showcase this functionality, let's look at the below code examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb1a55c0-5f07-4ace-8288-03e300a8d656",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "\n",
    "all_runs = mlflow.search_runs(search_all_experiments=True)\n",
    "print(all_runs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3478b6ea",
   "metadata": {},
   "source": [
    "Second, let's try filtering the runs for our really bad models:\n",
    "`metrics.loss > 0.8`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7d987be-e398-4079-a201-e7d8d6cf4123",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "\n",
    "bad_runs = mlflow.search_runs(\n",
    "    filter_string=\"metrics.loss > 0.8\", search_all_experiments=True\n",
    ")\n",
    "print(bad_runs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2466047",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "You'll notice that we now are displaying 2 runs instead of 10. Pretty\n",
    "easy, right?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc0fbad5",
   "metadata": {},
   "source": [
    "\n",
    "Now let's go over the search query syntax in more detail.\n",
    "\n",
    "\n",
    "\n",
    "### Search Query Syntax Deep Dive\n",
    "\n",
    "As noted above, MLflow search syntax is similar to SQL with a few\n",
    "notable exceptions.\n",
    "\n",
    "- The SQL `OR` keyword is not\n",
    "    supported.\n",
    "\n",
    "- For fields that contain special characters or start with numbers,\n",
    "    you need to wrap them in double quotes.\n",
    "\n",
    "    \n",
    "    \n",
    "        - Bad:  metrics.cross-entropy-loss < 0.5\n",
    "        + Good: metrics.\"cross-entropy-loss\" < 0.5\n",
    "\n",
    "        - Bad:  params.1st_iteration_timestamp = \"2022-01-01\"\n",
    "        + Good: params.\"1st_iteration_timestamp\" = \"2022-01-01\"\n",
    "\n",
    "    \n",
    "\n",
    "- For the SQL `IN` keyword, you must\n",
    "    surround the values of your list with single quotes.\n",
    "\n",
    "    \n",
    "    \n",
    "        - Bad:  attributes.run_id IN (\"5984a3488161440f92de9847e846b342\", \"babe221a676b4fa4b204f8240f2c4f14\")\n",
    "        + Good: attributes.run_id IN ('5984a3488161440f92de9847e846b342', 'babe221a676b4fa4b204f8240f2c4f14')\n",
    "\n",
    "    \n",
    "\n",
    "- For the SQL `IN` keyword, you can\n",
    "    only search the following fields:\n",
    "\n",
    "    - `datasets.name`\n",
    "\n",
    "    - `datasets.digest`\n",
    "\n",
    "    - `datasets.context`\n",
    "\n",
    "    - `attributes.run_id`\n",
    "\n",
    "- Non-None conditions for numeric fields are not supported e.g.\n",
    "    `metrics.accuracy != \"None\"` will\n",
    "    fail.\n",
    "\n",
    "Other than the that, the syntax should be intuitive to anyone who has\n",
    "used SQL. To assemble a single search condition, you must assemble an\n",
    "inequality using the following components...\n",
    "\n",
    "1.  **An MLflow field**: a metric, param, tag, dataset or run metadata.\n",
    "\n",
    "2.  **A comparator**: an inequality operator.\n",
    "\n",
    "- For numerics, MLflow supports =, !=, >, >=, <, and <=. Examples include:\n",
    "\n",
    "```\n",
    "    metrics.accuracy > 0.72\n",
    "    metrics.loss <= 0.15\n",
    "    metrics.accuracy != 0.15\n",
    "```\n",
    "\n",
    "- For strings, MLflow supports =, !=, LIKE (case-sensitive) and ILIKE (case-insensitive). Examples include:\n",
    "\n",
    "```\n",
    "    params.model = \"GPT-3\"\n",
    "    params.model LIKE \"GPT%\"\n",
    "    params.model ILIKE \"gpt%\"\n",
    "```\n",
    "\n",
    "- For sets, MLflow supports IN. Examples include:\n",
    "\n",
    "```\n",
    "    datasets.name IN ('custom', 'also custom', 'another custom name')\n",
    "    datasets.digest IN ('s8ds293b', 'jks834s2')\n",
    "    attributes.run_id IN ('5984a3488161440f92de9847e846b342')\n",
    "```\n",
    "\n",
    "3.  **A reference value**: a numeric value, string, or set of strings.\n",
    "\n",
    "Let's look at some more examples.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e032d640",
   "metadata": {},
   "source": [
    "\n",
    "### Example Queries\n",
    "\n",
    "In this section we will go over how to search by different categories of\n",
    "MLflow fields. For each category we provide a few sample queries. If you\n",
    "have executed the run creation script we provided, these queries should\n",
    "fetch certain runs but sometimes require modification for run-specific\n",
    "information, such as `start_time`.\n",
    "\n",
    "\n",
    "#### 1 - Searching By Metrics\n",
    "\n",
    "Metrics are quantitative measures typically used to evaluate the model's\n",
    "performance during or after training. Metrics can include values like\n",
    "accuracy, precision, recall, F1 score, etc., and can change over time as\n",
    "the model trains. They are logged manually via\n",
    "`mlflow.log_metric` or\n",
    "`mlflow.log_metrics` or automatically\n",
    "via autologging.\n",
    "\n",
    "To search for runs by filtering on metrics, you must include the\n",
    "`metrics` prefix in the left side of\n",
    "the inequality. Note that they are **stored as numbers**, so you must\n",
    "use numeric comparators.\n",
    "\n",
    "\n",
    "\n",
    "    metrics.accuracy > 0.72\n",
    "    metrics.\"accuracy\" > 0.72\n",
    "    metrics.loss <= 0.15\n",
    "    metrics.\"log-scale-loss\" <= 0\n",
    "    metrics.\"f1 score\" >= 0.5\n",
    "    metrics.accuracy > 0.72 AND metrics.loss <= 0.15\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88549549",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "#### 2 - Searching By Params\n",
    "\n",
    "Params are strings that typically represent the configuration aspects of\n",
    "the model. Parameters can include values like learning rate, batch size,\n",
    "and number of epochs. They are logged manually via\n",
    "`mlflow.log_param` or\n",
    "`mlflow.log_params` or automatically\n",
    "via autologging.\n",
    "\n",
    "To search for runs by filtering on params, you must include the\n",
    "`params` prefix in the left side of the\n",
    "inequality. Note that they are **stored as strings**, so you must use\n",
    "string comparators, such as `=` and\n",
    "`!=`.\n",
    "\n",
    "\n",
    "\n",
    "    params.batch_size = \"2\"\n",
    "    params.model LIKE \"GPT%\"\n",
    "    params.model ILIKE \"gPt%\"\n",
    "    params.model LIKE \"GPT%\" AND params.batch_size = \"2\"\n",
    "\n",
    "\n",
    "\n",
    "#### 3 - Searching By Tags\n",
    "\n",
    "Tags are metadata that typically provide additional context about the\n",
    "run. Tags can include values like user name, team, etc. They are logged\n",
    "manually via `mlflow.set_tag` or\n",
    "`mlflow.set_tags`. In addition, system\n",
    "tags, such as `mlflow.user`, are\n",
    "automatically logged.\n",
    "\n",
    "To search for runs by filtering on tags, you must include the\n",
    "`tags` or `mlflow` prefixes in the left side of the inequality. Note that\n",
    "tags are **stored as strings**, so you must use string comparators, such\n",
    "as `=` and `!=`.\n",
    "\n",
    "\n",
    "\n",
    "    tags.\"environment\" = \"notebook\"\n",
    "    tags.environment = \"notebook\"\n",
    "    tags.task = \"Classification\"\n",
    "    tags.task ILIKE \"classif%\"\n",
    "\n",
    "\n",
    "#### 4 - Searching By Dataset Information\n",
    "\n",
    "Datasets represent data used in model training or evaluation, including\n",
    "features, targets, predictions, and metadata such as the dataset's name,\n",
    "digest (hash) schema, profile, and source. They are logged via\n",
    "`mlflow.log_input` or automatically via\n",
    "autologging.\n",
    "\n",
    "To search for runs by filtering on dataset information, you must filter\n",
    "on one of the below fields\n",
    "\n",
    "1.  `datasets.name`, which is the\n",
    "    dataset's name.\n",
    "\n",
    "2.  `datasets.digest`, which is a\n",
    "    unique identifier for the dataset.\n",
    "\n",
    "3.  `datasets.context`, which\n",
    "    represents if the dataset is used for train, evaluation or test.\n",
    "\n",
    "Note that dataset information is **stored as strings**, so you must use\n",
    "string comparators, such as `=` and\n",
    "`!=`. Also note that datasets support\n",
    "set comparators, such as `IN`.\n",
    "\n",
    "\n",
    "\n",
    "    datasets.name LIKE \"custom\"\n",
    "    datasets.digest IN ('s8ds293b', 'jks834s2')\n",
    "    datasets.context = \"train\"\n",
    "\n",
    "\n",
    "#### 5 - Searching By Run's Metadata\n",
    "\n",
    "Run metadata are a variety of user-specified and system-generated\n",
    "attributes that provide additional context about the run.\n",
    "\n",
    "To search for runs by filtering on the metadata of runs, you must include the attributes prefix in the left side of the inequality. Note that run metadata can be either a string or a numeric depending on the attribute, so you must use the appropriate comparator.\n",
    "\n",
    "To search for runs by filtering on tags, you must include the\n",
    "`tags` or `mlflow` prefixes in the left side of the inequality. Note that\n",
    "tags are **stored as strings**, so you must use string comparators, such\n",
    "as `=` and `!=`.\n",
    "\n",
    "\n",
    "\n",
    "`Examples for Strings`\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    attributes.status = 'ACTIVE'\n",
    "    attributes.user_id LIKE 'user1'\n",
    "    attributes.run_name = 'my-run'\n",
    "    attributes.run_id = 'a1b2c3d4'\n",
    "    attributes.run_id IN ('a1b2c3d4', 'e5f6g7h8')\n",
    "\n",
    "\n",
    "\n",
    "`Examples for Numerics`\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    attributes.start_time >= 1664067852747\n",
    "    attributes.end_time < 1664067852747\n",
    "    attributes.created > 1664067852747\n",
    "\n",
    "\n",
    "\n",
    "#### 6 - Searching over a Set\n",
    "\n",
    "You can search for runs by filtering on a set of acceptable values via\n",
    "the `IN` keyword. As noted above, this\n",
    "is only supported for the following fields:\n",
    "\n",
    "- `datasets.{any_attribute}`\n",
    "\n",
    "- `attributes.run_id`\n",
    "\n",
    "\n",
    "\n",
    "    datasets.name IN ('custom', 'also custom')\n",
    "    datasets.digest IN ('s8ds293b', 'jks834s2')\n",
    "    attributes.run_id IN ('a1b2c3d4', 'e5f6g7h8')\n",
    "\n",
    "\n",
    "#### 7 - Chained Queries\n",
    "\n",
    "You can chain multiple queries together using the `AND` keyword. For example, to search for runs with a\n",
    "variety of conditions, you can use the following queries:\n",
    "\n",
    "    metrics.accuracy > 0.72 AND metrics.loss <= 0.15\n",
    "    metrics.accuracy > 0.72 AND metrics.batch_size != 0\n",
    "    metrics.accuracy > 0.72 AND metrics.batch_size != 0 AND attributes.run_id IN ('a1b2c3d4', 'e5f6g7h8')\n",
    "\n",
    "You can also apply multiple conditions on the same field, for example\n",
    "searching for all loss metrics `BETWEEEN` 0.1 and 0.15, inclusive:\n",
    "\n",
    "\n",
    "    metrics.loss <= 0.15 AND metrics.loss >= 0.1\n",
    "\n",
    "Finally, before moving on it's important to revisit that that you cannot\n",
    "use the `OR` keyword in your queries.\n",
    "\n",
    "\n",
    "\n",
    "#### 8 - Non-None Queries\n",
    "\n",
    "To search for runs where a field (only type string is supported) is not\n",
    "null, use the `field != \"None\"` syntax.\n",
    "For example, to search for runs where the batch\\_size is not null, you\n",
    "can use the following query:\n",
    "\n",
    "\n",
    "    params.batch_size != \"None\"\n",
    "\n",
    "\n",
    "\n",
    "Programmatically Searching Runs\n",
    "-------------------------------\n",
    "\n",
    "When scaling out to large production systems, typically you'll want to\n",
    "interact with your runs outside the MLflow UI. This can be done\n",
    "programmatically using the MLflow client APIs.\n",
    "\n",
    "\n",
    "### Python\n",
    "\n",
    "mlflow.client.MlflowClient.search_runs() or mlflow.search_runs() take the same arguments as the above UI examples and more! They return all the runs that match the specified filters.\n",
    "\n",
    "\n",
    "#### 1 - Complex Filter\n",
    "\n",
    "Python provides powerful ways to build these queries programmatically.\n",
    "Some tips:\n",
    "\n",
    "- For complex filters, specifically those with both single and double quotes, use multi-line strings or \\\\\" to escape the quotes.\n",
    "- When working with lists, use the `.join()` method to concatenate the list elements with a delimiter.\n",
    "- It's often most concise to use the fluent APIs, so below we demo\n",
    "    only with the fluent API.\n",
    "\n",
    "\n",
    "```\n",
    "import mlflow\n",
    "\n",
    "run_ids = [\"UPDATE_RUNID_HERE\", \"UPDATE_RUNID2_HERE\"]\n",
    "run_id_condition = \"'\" + \"','\".join(run_ids) + \"'\"\n",
    "\n",
    "complex_filter = f\"\"\"\n",
    "attributes.run_id IN ({run_id_condition})\n",
    "  AND metrics.loss > 0.3\n",
    "  AND metrics.\"f1 score\" < 0.5\n",
    "  AND params.model LIKE \"GPT%\"\n",
    "\"\"\"\n",
    "\n",
    "runs_with_complex_filter = mlflow.search_runs(\n",
    "    experiment_names=[\"search-run-guide\"],\n",
    "    filter_string=complex_filter,\n",
    ")\n",
    "print(runs_with_complex_filter)\n",
    "\n",
    "```\n",
    "\n",
    "The output will be a pandas DataFrame with the runs that match the\n",
    "specified filters, as shown below.\n",
    "\n",
    "```\n",
    "\n",
    "                                  run_id  ... tags.mlflow.runName\n",
    "    0  22db81f070f6413588641c8c343cdd72  ...   orderly-quail-568\n",
    "    1  c3680e37d0fa44eb9c9fb7828f6b5481  ...    melodic-lynx-301\n",
    "\n",
    "    [2 rows x 19 columns]\n",
    "```\n",
    "\n",
    "#### 2 - run\\_view\\_type\n",
    "\n",
    "The run_view_type parameter exposes additional filtering options, as noted in the mlflow.entities.ViewType enum. For example, if you want to filter only active runs, which is a dropdown in the UI, simply pass \n",
    "`run_view_type=ViewType.ACTIVE_ONLY`.\n",
    "\n",
    "\n",
    "```\n",
    "import mlflow\n",
    "from mlflow.entities import ViewType\n",
    "\n",
    "active_runs = mlflow.search_runs(\n",
    "    experiment_names=[\"search-run-guide\"],\n",
    "    run_view_type=ViewType.ACTIVE_ONLY,\n",
    "    order_by=[\"metrics.accuracy DESC\"],\n",
    ")\n",
    "```\n",
    "\n",
    "#### 2 - Ordering\n",
    "\n",
    "Another useful feature that is available in the search API is allowing\n",
    "for ordering of the returned search results. You can specify a list of\n",
    "columns of interest along with `DESC`\n",
    "or `ASC` in the `order_by` kwarg. Note that the `DESC` or `ASC` value is\n",
    "optional, so when the value is not provided, the default is\n",
    "`ASC`. Also note that the default\n",
    "ordering when the `order_by` parameter\n",
    "is omitted is to sort by `start_time DESC`, then `run_id ASC`.\n",
    "\n",
    "\n",
    "```\n",
    "import mlflow\n",
    "from mlflow.entities import ViewType\n",
    "\n",
    "active_runs_ordered_by_accuracy = mlflow.search_runs(\n",
    "    experiment_names=[\"search-run-guide\"],\n",
    "    run_view_type=ViewType.ACTIVE_ONLY,\n",
    "    order_by=[\"metrics.accuracy DESC\"],\n",
    ")\n",
    "active_runs_ordered_by_accuracy\n",
    "```\n",
    "\n",
    "A common use case is getting the top n results, for example, the top 5\n",
    "runs by accuracy. When combined with the `max_results` parameter, you can get the top `n` that match your query.\n",
    "\n",
    "\n",
    "```\n",
    "import mlflow\n",
    "from mlflow.entities import ViewType\n",
    "\n",
    "highest_accuracy_run = mlflow.search_runs(\n",
    "    experiment_names=[\"search-run-guide\"],\n",
    "    run_view_type=ViewType.ACTIVE_ONLY,\n",
    "    max_results=1,\n",
    "    order_by=[\"metrics.accuracy DESC\"],\n",
    ")\n",
    "\n",
    "```\n",
    "\n",
    "**Print output:**\n",
    "\n",
    "\n",
    "```\n",
    "highest_accuracy_run\n",
    "```\n",
    "\n",
    "#### 3 - Searching All Experiments\n",
    "\n",
    "Now you might be wondering how to search all experiments. It's as simple\n",
    "as specifying `search_all_experiments=True` and omitting the `experiment_ids` parameter.\n",
    "\n",
    "```\n",
    "import mlflow\n",
    "from mlflow.entities import ViewType\n",
    "\n",
    "model_of_interest = \"GPT-4\"\n",
    "gpt_4_runs_global = mlflow.search_runs(\n",
    "    filter_string=f\"params.model = '{model_of_interest}'\",\n",
    "    run_view_type=ViewType.ALL,\n",
    "    search_all_experiments=True,\n",
    ")\n",
    "```\n",
    "\n",
    "**Print output:**\n",
    "\n",
    "```\n",
    "gpt_4_runs_global\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bafaad7c-5766-46cd-ac0a-2bc94a5d9421",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "\n",
    "run_ids = [\"UPDATE_RUNID_HERE\", \"UPDATE_RUNID2_HERE\"]\n",
    "run_id_condition = \"'\" + \"','\".join(run_ids) + \"'\"\n",
    "\n",
    "complex_filter = f\"\"\"\n",
    "attributes.run_id IN ({run_id_condition})\n",
    "  AND metrics.loss > 0.3\n",
    "  AND metrics.\"f1 score\" < 0.5\n",
    "  AND params.model LIKE \"GPT%\"\n",
    "\"\"\"\n",
    "\n",
    "runs_with_complex_filter = mlflow.search_runs(\n",
    "    experiment_names=[\"search-run-guide\"],\n",
    "    filter_string=complex_filter,\n",
    ")\n",
    "print(runs_with_complex_filter)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6834e3c7-362e-4949-a93f-68e827bc12f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "from mlflow.entities import ViewType\n",
    "\n",
    "active_runs = mlflow.search_runs(\n",
    "    experiment_names=[\"search-run-guide\"],\n",
    "    run_view_type=ViewType.ACTIVE_ONLY,\n",
    "    order_by=[\"metrics.accuracy DESC\"],\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8466dd82-0865-4afe-8712-0da5a715b629",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "from mlflow.entities import ViewType\n",
    "\n",
    "active_runs_ordered_by_accuracy = mlflow.search_runs(\n",
    "    experiment_names=[\"search-run-guide\"],\n",
    "    run_view_type=ViewType.ACTIVE_ONLY,\n",
    "    order_by=[\"metrics.accuracy DESC\"],\n",
    ")\n",
    "active_runs_ordered_by_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaa1351e-a01d-47f3-9266-22c89832e41f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "from mlflow.entities import ViewType\n",
    "\n",
    "highest_accuracy_run = mlflow.search_runs(\n",
    "    experiment_names=[\"search-run-guide\"],\n",
    "    run_view_type=ViewType.ACTIVE_ONLY,\n",
    "    max_results=1,\n",
    "    order_by=[\"metrics.accuracy DESC\"],\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5db4533-474d-45dc-bdf3-85d71daf3a27",
   "metadata": {},
   "outputs": [],
   "source": [
    "highest_accuracy_run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "595f9f5f-d3bc-4434-8aee-a3ce9e9d0158",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "from mlflow.entities import ViewType\n",
    "\n",
    "model_of_interest = \"GPT-4\"\n",
    "gpt_4_runs_global = mlflow.search_runs(\n",
    "    filter_string=f\"params.model = '{model_of_interest}'\",\n",
    "    run_view_type=ViewType.ALL,\n",
    "    search_all_experiments=True,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ea2b775-5a90-44bb-83dd-059c444df352",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt_4_runs_global"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
